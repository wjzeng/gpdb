<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic25">
    <title>Reading and Writing Custom-Formatted HDFS Data with gphdfs (Deprecated)</title>
    <body>
        <note>The <codeph>gphdfs</codeph> external table protocol is deprecated and will be
        removed in the next major release of Greenplum Database.</note>
        <p>Use MapReduce and the <codeph>CREATE EXTERNAL TABLE</codeph> command to read and write
            data with custom formats on HDFS.</p>
        <p>To read custom-formatted data:</p>
        <ol>
            <li id="du213746">Author and run a MapReduce job that creates a copy of the data in a
                format accessible to Greenplum Database.</li>
            <li id="du213767">Use <codeph>CREATE EXTERNAL TABLE</codeph> to read the data into
                Greenplum Database.</li>
        </ol>
        <p>See <xref href="#topic26"/>.</p>
        <p>To write custom-formatted data:</p>
        <ol>
            <li id="du213888">Write the data.</li>
            <li id="du213900">Author and run a MapReduce program to convert the data to the custom
                format and place it on the Hadoop Distributed File System. </li>
        </ol>
        <p>See <xref href="#topic29"/>.</p>
        <p>MapReduce code is written in Java. Greenplum provides Java APIs for use in the MapReduce
            code. The Javadoc is available in the <codeph>$GPHOME/docs</codeph> directory. To view
            the Javadoc, expand the file <codeph>gnet-1.2-javadoc.tar</codeph> and open
                <codeph>index.html</codeph>. The Javadoc documents the following packages:</p>
        <p>
            <codeblock>com.emc.greenplum.gpdb.hadoop.io
com.emc.greenplum.gpdb.hadoop.mapred
com.emc.greenplum.gpdb.hadoop.mapreduce.lib.input
com.emc.greenplum.gpdb.hadoop.mapreduce.lib.output
</codeblock>
        </p>
        <p>The HDFS cross-connect packages contain the Java library, which contains the packages
                <codeph>GPDBWritable</codeph>, <codeph>GPDBInputFormat</codeph>, and
                <codeph>GPDBOutputFormat</codeph>. The Java packages are available in
                <codeph>$GPHOME/lib/hadoop</codeph>. Compile and run the MapReduce job with the
            cross-connect package. For example, compile and run the MapReduce job with
                <codeph>hdp-gnet-1.2.0.0.jar</codeph> if you use the HDP
            distribution of Hadoop.</p>
        <p>To make the Java library available to all Hadoop users, the Hadoop cluster administrator
            should place the corresponding <codeph>gphdfs</codeph> connector jar in the
                <codeph>$HADOOP_HOME/lib</codeph> directory and restart the job tracker. If this is
            not done, a Hadoop user can still use the <codeph>gphdfs</codeph> connector jar; but
            with the <i>distributed cache</i> technique. </p>
    </body>
    <topic id="topic26">
        <title>Example 1 - Read Custom-Formatted Data from HDFS</title>
        <body>
            <p>The sample code makes the following assumptions.</p>
            <ul id="ul_xxf_bvd_dbb">
                <li id="du190126">The data is contained in HDFS directory
                        <codeph>/demo/data/temp</codeph> and the name node is running on port
                    8081.</li>
                <li id="du191022">This code writes the data in Greenplum Database format to
                        <codeph>/demo/data/MRTest1</codeph> on HDFS.</li>
                <li id="du190109">The data contains the following columns, in order.<ol
                        id="ol_yxf_bvd_dbb">
                        <li id="du214384">A long integer</li>
                        <li id="du214693">A Boolean</li>
                        <li id="du214544">A text string</li>
                    </ol>
                </li>
            </ul>
        </body>
        <topic id="topic27">
            <title>Sample MapReduce Code</title>
            <body>
                <codeblock>import com.emc.greenplum.gpdb.hadoop.io.GPDBWritable;
import com.emc.greenplum.gpdb.hadoop.mapreduce.lib.input.GPDBInputFormat;
import com.emc.greenplum.gpdb.hadoop.mapreduce.lib.output.GPDBOutputFormat;
import java.io.*;
import java.util.*;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.output.*;
import org.apache.hadoop.mapreduce.lib.input.*;
import org.apache.hadoop.util.*;

public class demoMR {

/*
 * Helper routine to create our generic record. This section shows the
 * format of the data. Modify as necessary. 
 */
 public static GPDBWritable generateGenericRecord() throws
      IOException {
 int[] colType = new int[3];
 colType[0] = GPDBWritable.BIGINT;
 colType[1] = GPDBWritable.BOOLEAN;
 colType[2] = GPDBWritable.VARCHAR;
 
  /*
   * This section passes the values of the data. Modify as necessary. 
   */ 
  GPDBWritable gw = new GPDBWritable(colType); 
  gw.setLong (0, (long)12345);  
  gw.setBoolean(1, true); 
  gw.setString (2, "abcdef");
  return gw; 
} 

/* 
 * DEMO Map/Reduce class test1
 * -- Regardless of the input, this section dumps the generic record
 * into GPDBFormat/
 */
 public static class Map_test1 
     extends Mapper&lt;LongWritable, Text, LongWritable, GPDBWritable&gt; {
 
  private LongWritable word = new LongWritable(1);

  public void map(LongWritable key, Text value, Context context) throws
       IOException { 
    try {
      GPDBWritable gw = generateGenericRecord();
      context.write(word, gw); 
      } 
      catch (Exception e) { 
        throw new IOException (e.getMessage()); 
      } 
    }
  }

  Configuration conf = new Configuration(true);
  Job job = new Job(conf, "test1");
  job.setJarByClass(demoMR.class);
  job.setInputFormatClass(TextInputFormat.class);
  job.setOutputKeyClass (LongWritable.class);
  job.setOutputValueClass (GPDBWritable.class);
  job.setOutputFormatClass(GPDBOutputFormat.class);
  job.setMapperClass(Map_test1.class);
  FileInputFormat.setInputPaths (job, new Path("/demo/data/tmp"));
  GPDBOutputFormat.setOutputPath(job, new Path("/demo/data/MRTest1"));
  job.waitForCompletion(true);
}</codeblock>
            </body>
        </topic>
        <topic id="topic28">
            <title> Run CREATE EXTERNAL TABLE</title>
            <body>
                <p>The Hadoop location corresponds to the output path in the MapReduce job.</p>
                <p>
                    <codeblock>=# CREATE EXTERNAL TABLE demodata 
   LOCATION ('gphdfs://hdfshost-1:8081/demo/data/MRTest1') 
   FORMAT 'custom' (formatter='gphdfs_import');
</codeblock>
                </p>
            </body>
        </topic>
    </topic>
    <topic id="topic29">
        <title>Example 2 - Write Custom-Formatted Data from Greenplum Database to HDFS</title>
        <body>
            <p>The sample code makes the following assumptions.</p>
            <ul id="ul_ghb_2vd_dbb">
                <li id="du190948">The data in Greenplum Database format is located on the Hadoop
                    Distributed File System on <codeph>/demo/data/writeFromGPDB_42</codeph> on port
                    8081.</li>
                <li id="du190949">This code writes the data to <codeph>/demo/data/MRTest2</codeph>
                    on port 8081.</li>
            </ul>
            <ol id="ol_hhb_2vd_dbb">
                <li id="du190951">Run a SQL command to create the writable table.<p>
                        <codeblock>=# CREATE WRITABLE EXTERNAL TABLE demodata 
   LOCATION ('gphdfs://hdfshost-1:8081/demo/data/MRTest2') 
   FORMAT 'custom' (formatter='gphdfs_export');
</codeblock>
                    </p>
                </li>
                <li id="du191051">Author and run code for a MapReduce job. Use the same import
                    statements shown in <xref href="#topic26"/>.</li>
            </ol>
        </body>
    </topic>
    <topic id="topic30">
        <title> Sample MapReduce Code </title>
        <body>
            <codeblock>/*
 * DEMO Map/Reduce class test2
 * -- Convert GPDBFormat back to TEXT
 */
public static class Map_test2 extends Mapper&lt;LongWritable, GPDBWritable,
  Text, NullWritable&gt; { 
  public void map(LongWritable key, GPDBWritable value, Context context )
    throws IOException {
    try {
      context.write(new Text(value.toString()), NullWritable.get());
    } catch (Exception e) { throw new IOException (e.getMessage()); }
  }
}

public static void runTest2() throws Exception{
Configuration conf = new Configuration(true);
 Job job = new Job(conf, "test2");
 job.setJarByClass(demoMR.class);
 job.setInputFormatClass(GPDBInputFormat.class);
 job.setOutputKeyLClass (Text.class);
 job.setOutputValueClass(NullWritable.class);
 job.setOutputFormatClass(TextOutputFormat.class);
 job.setMapperClass(Map_test2.class);
     GPDBInputFormat.setInputPaths (job, 
     new Path("/demo/data/writeFromGPDB_42"));
 GPDBOutputFormat.setOutputPath(job, new Path("/demo/data/MRTest2"));
 job.waitForCompletion(true);
     
}</codeblock>
        </body>
    </topic>
</topic>
