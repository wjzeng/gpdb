-- Misc tests related to dispatching queries to segments.
CREATE DATABASE dispatch_test_db;
\c dispatch_test_db;
CREATE EXTENSION IF NOT EXISTS gp_inject_fault;
-- Mask out the whoami message
-- start_matchsubs
-- m/^ \(seg\d .*:\d+\)/
-- s/^ \(seg\d .*:\d+\)//
-- m/^DETAIL:  Internal error: No motion listener port \(seg\d.*:.*\)/
-- s/^DETAIL:  Internal error: No motion listener port \(seg\d.*:.*\)/DETAIL:  Internal error: No motion listener port/
-- end_matchsubs
-- Test quoting of GUC values and database names when they're sent to segments
-- set_config() used to have quoting problems as well when dispatching to
-- segments.
DROP TABLE IF EXISTS should_be_visible;
NOTICE:  table "should_be_visible" does not exist, skipping
CREATE TABLE should_be_visible();
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause, and no column type is suitable for a distribution key. Creating a NULL policy entry.
CREATE TABLE distribute_table_oid_to_all_segs (distribution_key INT, tbl OID) DISTRIBUTED RANDOMLY;
INSERT INTO distribute_table_oid_to_all_segs SELECT *, 'public.should_be_visible'::regclass FROM generate_series(1, 10);
-- Spin up any gangs necessary to handle the SELECT DISTINCT so that they
-- receive the new search_path setting from the set_config() dispatch.
SELECT DISTINCT pg_catalog.pg_table_is_visible(tbl) FROM distribute_table_oid_to_all_segs;
 pg_table_is_visible 
---------------------
 t
(1 row)

-- Now change search_path. This setting should not affect the visibility of the
-- public schema if it's dispatched correctly.
SELECT set_config('search_path', 'pg_catalog,public', false);
    set_config     
-------------------
 pg_catalog,public
(1 row)

-- Check that our test table is still visible.
SELECT DISTINCT pg_catalog.pg_table_is_visible(tbl) FROM distribute_table_oid_to_all_segs;
 pg_table_is_visible 
---------------------
 t
(1 row)

RESET search_path;
-- There used to be a bug in the quoting when the search_path setting was sent
-- to the segment. It was not easily visible when search_path was set with a
-- SET command, only when the setting was sent as part of the startup packet.
-- Set search_path as a per-user setting so that we can test that.
CREATE DATABASE "dispatch test db";
ALTER DATABASE "dispatch test db" SET search_path="my schema",public;
NOTICE:  schema "my schema" does not exist
\c "dispatch test db"
CREATE SCHEMA "my schema";
-- Create a table with the same name in both schemas, "my schema" and public.
CREATE TABLE "my table" (t text);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 't' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO "my table" VALUES ('myschema.mytable');
CREATE TABLE public."my table" (t text);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 't' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
INSERT INTO public."my table" VALUES ('public.mytable');
SELECT t as unquoted FROM "my table";
     unquoted     
------------------
 myschema.mytable
(1 row)

SELECT t as myschema FROM "my schema"."my table";
     myschema     
------------------
 myschema.mytable
(1 row)

SELECT t as public FROM public."my table";
     public     
----------------
 public.mytable
(1 row)

DROP TABLE "my table";
DROP TABLE public."my table";
-- Create another table with the same name. To make sure the DROP worked
-- and dropped the correct table.
CREATE TABLE "my table" (id integer);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'id' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
DROP TABLE "my table";
-- Clean up
\c dispatch_test_db
DROP DATABASE "dispatch test db";
-- Test gp_max_plan_size limit
set gp_max_plan_size='10 kB';
create table dispatchtesttab(t text);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 't' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
select * from dispatchtesttab where t = repeat('x', 1024*11);
ERROR:  Query plan size limit exceeded, current size: 11KB, max allowed size: 10KB
HINT:  Size controlled by gp_max_plan_size
--
-- test QD will report failure if QE fails to send its motion_listener back
-- during backend initialization
--
select gp_inject_fault('send_qe_details_init_backend', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- inject a 'skip' fault before QE sends its motion_listener
select gp_inject_fault('send_qe_details_init_backend', 'skip', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- terminate exiting QEs first
\c dispatch_test_db
-- verify failure will be reported
SELECT 1 FROM gp_dist_random('gp_id');
ERROR:  failed to acquire resources on one or more segments
DETAIL:  Internal error: No motion listener port (seg0 127.0.0.1:25432)
-- reset fault injector
select gp_inject_fault('send_qe_details_init_backend', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

--
-- Test suit : test gang creation and commands dispatching 
--
--start_ignore
drop table if exists dispatch_test;
NOTICE:  table "dispatch_test" does not exist, skipping
drop table if exists dispatch_test_t1;
NOTICE:  table "dispatch_test_t1" does not exist, skipping
drop table if exists dispatch_test_t2;
NOTICE:  table "dispatch_test_t2" does not exist, skipping
drop table if exists dispatch_test_t3;
NOTICE:  table "dispatch_test_t3" does not exist, skipping
--end_ignore
create table dispatch_test as select i as c1 from generate_series(1, 10) i;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
create table dispatch_test_t1 (c1 int, c2 int, c3 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
create table dispatch_test_t2 (c1 int, c2 int, c3 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
create table dispatch_test_t3 (c1 int, c2 int, c3 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into dispatch_test_t1 values (1,1,2);
insert into dispatch_test_t2 values (2,1,2);
insert into dispatch_test_t3 values (3,1,2);
CREATE OR REPLACE FUNCTION cleanupAllGangs() RETURNS BOOL
AS '@abs_builddir@/regress@DLSUFFIX@', 'cleanupAllGangs' LANGUAGE C;
-- check if segments has backend running
CREATE OR REPLACE FUNCTION hasBackendsExist(int) RETURNS BOOL 
AS '@abs_builddir@/regress@DLSUFFIX@', 'hasBackendsExist' LANGUAGE C;
-- check if QD has reusable gangs
CREATE OR REPLACE FUNCTION hasGangsExist() RETURNS BOOL
AS '@abs_builddir@/regress@DLSUFFIX@', 'hasGangsExist' LANGUAGE C;
-- disable debug linger to get immediate feedback from FATAL errors.
set gp_debug_linger to 0;
-- test log debug related code within dispatch
set gp_log_gang to debug;
set log_min_messages to DEBUG;
-- Case 1.1
-- A segment in recovery mode, writer gang retry gp_gang_creation_retry_count times and finally success
-- set maximum retry time to 120 seconds, this should be long enough for segment
-- recovery back. otherwise it should be bug somewhere
set gp_gang_creation_retry_count to 120;
set gp_gang_creation_retry_timer to 1000;
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- trigger fault and report segment 0 in recovery for 5 times
select gp_inject_fault_new('process_startup_packet', 'skip', '', 'dispatch_test_db', '', 1, 5, 0, 2::smallint);
NOTICE:  Success:
 gp_inject_fault_new 
---------------------
 t
(1 row)

select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- should success after retry
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
 c1 | c2 | c3 | c1 | c2 | c3 | c1 | c2 | c3 
----+----+----+----+----+----+----+----+----
  1 |  1 |  2 |  2 |  1 |  2 |  3 |  1 |  2
(1 row)

select gp_inject_fault('process_startup_packet', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- Case 1.2
-- A segment in recovery mode for long time, writer gang retry gp_gang_creation_retry_count times and finally failed
-- set maximun retry time to 0.4s, so we can test if gp_gang_creation_retry_count
-- is actually work
set gp_gang_creation_retry_count to 2;
set gp_gang_creation_retry_timer to 200;
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- trigger fault and put segment 0 into recovery mode
select gp_inject_fault_new('process_startup_packet', 'skip', '', 'dispatch_test_db', '', 1, 5, 0, 2::smallint);
NOTICE:  Success:
 gp_inject_fault_new 
---------------------
 t
(1 row)

select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- should failed after 2 times
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
ERROR:  failed to acquire resources on one or more segments
DETAIL:  segment(s) are in recovery mode
set gp_gang_creation_retry_count to 10;
-- should success and process_startup_packet will be invalid after this query
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
 c1 | c2 | c3 | c1 | c2 | c3 | c1 | c2 | c3 
----+----+----+----+----+----+----+----+----
  1 |  1 |  2 |  2 |  1 |  2 |  3 |  1 |  2
(1 row)

select gp_inject_fault('process_startup_packet', 'reset', 2);
NOTICE:  Success:  (seg0 10.153.101.106:25432 pid=373925)
 gp_inject_fault 
-----------------
 t
(1 row)

--start_ignore
-- enlarge the retry count
set gp_gang_creation_retry_count to 128 ;
-- this will block until segment 0 recovery back, or report an error
-- after 24 seconds.
select 'wait recovery finish' from gp_dist_random('gp_id');
       ?column?       
----------------------
 wait recovery finish
 wait recovery finish
 wait recovery finish
(3 rows)

--end_ignore
-- cleanup all reusable gangs 
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- expect no zombie backends left on segments 
select sum(case when hasBackendsExist(30)='t' then 1 else 0 end) > 0 as hasbackends from gp_dist_random('gp_id');
 hasbackends 
-------------
 f
(1 row)

-- should success 
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
 c1 | c2 | c3 | c1 | c2 | c3 | c1 | c2 | c3 
----+----+----+----+----+----+----+----+----
  1 |  1 |  2 |  2 |  1 |  2 |  3 |  1 |  2
(1 row)

-- Case 1.3
-- segment has non in-recovery-mode errors
-- when creating writer gang, an error reported and all gangs should be cleaned.
-- when creating reader gang, an error reported and only current gang is cleaned.
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- Segment 0 should report an error upon getting a request.  The
-- _new() API ensures that the fault is triggered exactly once.
select gp_inject_fault_new('send_qe_details_init_backend', 'error', 2);
NOTICE:  Success:
 gp_inject_fault_new 
---------------------
 t
(1 row)

-- expect failure
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
ERROR:  failed to acquire resources on one or more segments
DETAIL:  FATAL:  fault triggered, fault name:'send_qe_details_init_backend' fault type:'error'
 (seg0 127.0.0.1:25432)
-- expect no resuable gang exist
select * from hasGangsExist();
 hasgangsexist 
---------------
 f
(1 row)

-- expect no zombie backends left on segments.
select sum(case when hasBackendsExist(30)='t' then 1 else 0 end) > 0 as hasbackends from gp_dist_random('gp_id');
 hasbackends 
-------------
 f
(1 row)

-- cleanupAllGangs();
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

select gp_inject_fault('send_qe_details_init_backend', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- segment 0 report an error when get the second request (reader gang creation request)
select gp_inject_fault('send_qe_details_init_backend', 'error', '', '', '', 3, 0, 2::smallint);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- expect failure
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
ERROR:  failed to acquire resources on one or more segments
DETAIL:  FATAL:  fault triggered, fault name:'send_qe_details_init_backend' fault type:'error'
 (seg0 127.0.0.1:25432)
-- expect resuable gang exist
select * from hasGangsExist();
 hasgangsexist 
---------------
 t
(1 row)

-- expect QEs exist.
select sum(case when hasBackendsExist(0)='t' then 1 else 0 end) > 0 as hasbackends from gp_dist_random('gp_id');
 hasbackends 
-------------
 t
(1 row)

select gp_inject_fault('send_qe_details_init_backend', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- Case 1.4
-- Test createGang timeout.
-- gp_segment_connect_timeout = 0 : wait forever
-- gp_segment_connect_timeout = 1 : wait 1 second
set gp_segment_connect_timeout to 1;
select gp_inject_fault_new('process_startup_packet', 'suspend', '', 'dispatch_test_db', '', 1, 1, 0, 2::smallint);
NOTICE:  Success:
 gp_inject_fault_new 
---------------------
 t
(1 row)

select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

-- expect timeout failure
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
ERROR:  failed to acquire resources on one or more segments
DETAIL:  timeout expired
 (seg0 127.0.0.1:25432)
select gp_inject_fault('process_startup_packet', 'resume', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

select gp_inject_fault('process_startup_packet', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

set gp_segment_connect_timeout to 0;
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

select gp_inject_fault('process_startup_packet', 'sleep', '', '', '', 1, 1, 2::smallint);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- expect success 
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
 c1 | c2 | c3 | c1 | c2 | c3 | c1 | c2 | c3 
----+----+----+----+----+----+----+----+----
  1 |  1 |  2 |  2 |  1 |  2 |  3 |  1 |  2
(1 row)

select gp_inject_fault('process_startup_packet', 'reset', 2);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- Case 1.5
-- query was cancelled when dispatching commands to one gang.
-- query should be cancelled as expected.
-- must set log_min_messages to default when using interrupt, there is a bug in fault injection.
set log_min_messages to default;
select gp_inject_fault('after_one_slice_dispatched', 'interrupt', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- should fail and report error
select * from dispatch_test_t1, dispatch_test_t2, dispatch_test_t3
where dispatch_test_t1.c2 = dispatch_test_t2.c2 and dispatch_test_t2.c3 = dispatch_test_t3.c3;
ERROR:  canceling statement due to user request
select gp_inject_fault('after_one_slice_dispatched', 'reset', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- test logging of gang management
SET gp_log_gang = 'debug';
-- test INFO raised from segments with various kinds of fields
CREATE OR REPLACE FUNCTION udf_raise_info() RETURNS BOOL
AS '@abs_builddir@/regress@DLSUFFIX@', 'gangRaiseInfo' LANGUAGE C;
SELECT udf_raise_info() FROM gp_dist_random('gp_id') WHERE gp_segment_id = 0;
INFO:  testing hook function MPPnoticeReceiver  (seg0 slice1 127.0.0.1:25432 pid=92553)
DETAIL:  this test aims at covering code paths not hit before
HINT:  no special hint
CONTEXT:  PL/C function defined in regress.c
 udf_raise_info 
----------------
 t
(1 row)

--
-- Error out when dispatching DTX command to busy primary writer gang
--
CREATE TABLE dtx_dispatch_t AS SELECT i AS c1 FROM generate_series(1,10) i;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CREATE or REPLACE FUNCTION dtx_dispatch_f(integer)
RETURNS INTEGER
AS
$$
BEGIN
        return $1 + 1;
EXCEPTION
        WHEN division_by_zero THEN
        RAISE NOTICE 'caught division_by_zero';
        return 0;
END
$$
LANGUAGE plpgSQL READS SQL DATA;
SELECT dtx_dispatch_f(foo.c1) FROM (SELECT c1 FROM dtx_dispatch_t WHERE c1='1' limit 1) foo;
ERROR:  query plan with multiple segworker groups is not supported
HINT:  dispatching DTX commands to a busy gang
CONTEXT:  PL/pgSQL function "dtx_dispatch_f" line 1 during statement block entry
DROP FUNCTION dtx_dispatch_f(integer);
DROP TABLE dtx_dispatch_t;
-- Test interconnect is shut down under portal failure
CREATE OR REPLACE FUNCTION numActiveMotionConns() RETURNS INT
AS '@abs_builddir@/regress@DLSUFFIX@', 'numActiveMotionConns' LANGUAGE C;
CREATE TABLE foo_test AS SELECT i AS c1 FROM generate_series(1, 10) i;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause. Creating a NULL policy entry.
SELECT c1/0 FROM foo_test WHERE c1 = 1;
ERROR:  division by zero  (seg1 slice1 11.111.111.116:25433 pid=166651)
SELECT numActiveMotionConns();
 numactivemotionconns
----------------------
                    0
(1 row)

BEGIN;
DECLARE C1 CURSOR FOR SELECT * FROM foo_test;
FETCH BACKWARD 1 FROM C1;
ERROR:  backward scan is not supported in this version of Greenplum Database
END;
SELECT numActiveMotionConns();
 numactivemotionconns
----------------------
                    0
(1 row)

DROP FUNCTION numActiveMotionConns();
DROP TABLE foo_test;
--
-- Test dangling Gang would be destroyed if interrupted during the creation
--
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

select gp_inject_fault('gang_created', 'reset', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

-- The _new() API ensures that the fault is triggered exactly once.
select gp_inject_fault_new('gang_created', 'error', 1);
NOTICE:  Success:
 gp_inject_fault_new 
---------------------
 t
(1 row)

select 1 from gp_dist_random('gp_id') limit 1;
ERROR:  fault triggered, fault name:'gang_created' fault type:'error'
-- if previous gang is not destroyed, snapshot collision would happen
select 1 from gp_dist_random('gp_id') limit 1;
 ?column? 
----------
        1
(1 row)

select gp_inject_fault('gang_created', 'reset', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

--
-- Test when error happens in createGang, it can correctly clean up
--
select cleanupAllGangs();
 cleanupallgangs 
-----------------
 t
(1 row)

select gp_inject_fault('gang_created', 'reset', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

select gp_inject_fault('gang_created', 'skip', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

select 1 from gp_dist_random('gp_id') limit 1;
ERROR:  failed to acquire resources on one or more segments
select gp_inject_fault('gang_created', 'reset', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

select 1 from gp_dist_random('gp_id') limit 1;
 ?column? 
----------
        1
(1 row)

--
-- Test that an error happens after a big command is dispatched.
--
select gp_inject_fault('after_one_slice_dispatched', 'error', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

select * from gp_dist_random('gp_id')
	where gpname > (select * from repeat('sssss', 10000000));
ERROR:  fault triggered, fault name:'after_one_slice_dispatched' fault type:'error'
select gp_inject_fault('after_one_slice_dispatched', 'reset', 1);
NOTICE:  Success:
 gp_inject_fault 
-----------------
 t
(1 row)

select * from gp_dist_random('gp_id')
	where gpname > (select * from repeat('sssss', 10000000));
 gpname | numsegments | dbid | content 
--------+-------------+------+---------
(0 rows)

--
-- Test portal cleanup
-- prepare a query contains init plan, main plan create a unnamed portal
-- and will need multiple slices , init plan also need to allocated multiple
-- slices in a named portal (eg: a cursor). The executing order is:
-- 1. gangs of main plan is pre-assigned 2. init plan is executed 3. main plan
-- is executed. This test it meant to test that cleanup of the named portal in
-- the end of step2 will not affect the pre-allocated gangs of the main plan.
--
create table foo (c1 int, c2 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
create table bar (c1 int, c2 int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
create or replace function foo_func()
returns int
as
$BODY$
declare
t_record record;
BEGIN
	drop table if exists tmp1;
	create temp table tmp1 as select foo.c1, bar.c2 from foo, bar;
	for t_record in (select count(*) from foo, bar)
	loop
		NULL;
	end loop;
	return 1;
END;
$BODY$
language plpgsql volatile;
set gp_cached_segworkers_threshold to 1;
select count(*) from foo, bar where exists (select foo_func());
NOTICE:  table "tmp1" does not exist, skipping
CONTEXT:  SQL statement "drop table if exists tmp1"
PL/pgSQL function "foo_func" line 4 at SQL statement
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'c1' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
CONTEXT:  SQL statement "create temp table tmp1 as select foo.c1, bar.c2 from foo, bar"
PL/pgSQL function "foo_func" line 5 at SQL statement
 count 
-------
     0
(1 row)

reset gp_cached_segworkers_threshold;
\c regression
DROP DATABASE dispatch_test_db;
